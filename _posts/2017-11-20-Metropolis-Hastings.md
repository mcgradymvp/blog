---
layout: single
title: Metropolis-Hastings Algorithm
use_math: true
tags: Bayesian Statistics MCMC
category: Markov chain Monte Carlo
excerpt: An introduction to Metropolis-Hastings Algorithm
---


In Bayesian statistics, we treat the unknown parameter $\theta$ as a random variable, and estimate $\theta$ by a posterior distribution, which is proportional to the product of prior and likelihood. When conjugate prior is applied, the analytical form of posterior can be derived since it has the same form as prior. In some situations, we are able to perform a numerical integration of normalising constant. However, in most cases, the analytical and numerical results can be hardly obtained, especially in high-dimensional cases. In such cases, Monte Carlo methods play an important role. In this post, we are about to give an introduction of Metropolis-Hastings Algorithm.

## Metropolis-Hastings Algorithm
Markov chain Monte Carlo has been widely used to generate random samples from a probability density function, which in intractable to generate directly. The method usually generates a Markov chain with stationary distribution close to our target distribution.

Metropolis-Hastings Algorithm, which was proposed by Metropolis and Rosenbluth[\[1\]](#Metro) and later generalized by W.K. Hastings[\[2\]](#Hastings), is probably the best known MCMC methods.

Suppose we are interested in density function $f(x)$. Given an initial state $x_0$ and a proposal distribution $q(\cdot, x)$ depend on $x$. The Metropolis-Hasting Algorithm performs the following steps for enough number of iterations until the Markov chain converges:
- At state $x_i$, we propose a new state $y_i$ from the proposal distribution $q(\cdot, x_i)$
- Compute the acceptance rate $\alpha=\min(\frac{f(y_i)q(y_i, x_i)}{f(x_i)q(x_i, y_i)}, 1)$
- Generate $u\sim U(0, 1)$. If $u<\alpha$, then set $x_{i+1} = y_i$, otherwise set $x_{i+1}=x_i$.

#### Some notes about Metropolis-Hastings Algorithm
1. The acceptance rate $\alpha$ requires only the ratio of target distribution, which cancels out the normalisation term in posterior distribution. That's why in MH algorithm, we only need an unnormalised form of target distribution.
2. The proposal distribution has a great influence in the convergence of Markov chain. The markov chain converges to stationary distribution faster when the proposal density is closer to target distribution.
3. We accept the proposal state with the probability of $\alpha$ in order to satisfy the detailed balance property. We will discuss the reason for this in later section.


## Why Would Metropolis-Hastings Algorithm Work?
According to the definition of [Markov chain](https://en.wikipedia.org/wiki/Markov_chain#Markov_property), the sequence generated by Metropolis-Hastings Algorithm is indeed a Markov chain. However, we might ask, why would this Markov chain generated converges to our target distribution? We divide this question into two aspects: (1)the Markov chain has a unique stationary distribution and (2)the unique stationary distribution is exactly our target $f(x)$.

The first aspect is proved by ergodicity of Markov chain, that is, if a Markov chain is irreducible, aperiodic and positive recurrent, then a unique stationary distribution exists. 

What's remaining is to prove the stationary distribution equals to the target distribution. To see this, we first consider the detailed balance condition(reversibility) for Markov chain. 
> A Markov chain is said to be **reversible** with distribution $\pi$, and transition matrix $P$, if $\pi_ip_{ij}=\pi_jp_{ji}$ for all $i, j$.

The detailed balance property also implies stationarity. If a Markov chain with transition matrix $P$ satisfies detailed balance with distribution $\pi$, then $\pi$ is a stationary distribution. This can be easily proved by 

$$\begin{eqnarray}
\sum_j\pi_jP_{ji} &=& \sum_j\pi_iP_{ij} \nonumber \\
&=&\pi_i\sum_jP_{ij} \nonumber \\
&=&\pi_i \nonumber
\end{eqnarray}$$ 

Now we prove the Markov chain generated by Metropolis-Hastings algorithm satisfies detailed balance condition, with the target density $f(x)$. If we assume that $\frac{f(j)q_{ji}}{f(i)q_{ij}}\leq 1$, then

$$P_{ji} = q_{ji}\min(\frac{f(i)q_{ij}}{f(j)q_{ji}}, 1) = q_{ji}$$

and 

$$P_{ij} = q_{ij}\min(\frac{f(j)q_{ji}}{f(i)q_{ij}}, 1) = q_{ij}\frac{f(j)q_{ji}}{f(i)q_{ij}} = \frac{f(j)q_{ji}}{f(i)}$$

which implies the detailed balance condition:

$$P_{ij} = \frac{f(j)P_{ji}}{f(i)} \Rightarrow P_{ij}f(i) = P_{ji}f(j)$$

Thus, the Metropolis-Hastings algorithm generates a Markov chain, with a unique stationary distribution, which is exactly our target density $f(x)$

## Implementation of Metropolis-Hastings Algorithm
We first use Metropolis-Hastings Algorithm to generate a sequence of samples from a [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution). Here our target distribution is Gamma(1.2, 2.9), and we chose 

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Define a target distribution Gamma(1.2, 2.9)
def target(x):
	return stats.gamma.pdf(x, 1.2, scale=1/2.9)

# Define a proposal ditribution $q(\cdot, x)$
def q(y, x):
	sigma = 0.5
	return stats.norm.pdf(y, x, sigma)

# Metropolis-Hastings Algorithm
def MHA(nsteps, fun, X0):
	X = np.zeros(nsteps)
	X[0] = X0
	for i in range(nsteps-1):
		Y = np.random.normal(X[i], sigma, 1)[0]
		U = np.random.uniform(0, 1, 1)[0]
		alpha = min(1, fun(Y)*q(X[i], Y)/(fun(X[i])*q(Y, X[i])))
		if U <= alpha:
			X[i+1] = Y
		else:
			X[i+1] = X[i]
	return X

mcmc_sequence = MHA(5000, target, 1)
x = np.linspace(0, 2, 10000)
plt.plot(x, target(x), label='True density')
plt.hist(mcmc_sequence, bins=30, density=True, label='MCMC estimates')
plt.legend()
plt.show()
```

<center><img src="{{site.url}}/images/MH_toy1.png" alt="Metropolis-Hastings Toye Example" title="Metropolis-Hastings Toy Example" style="width: 500px;"/></center>

As we can see from the figure, samples generated by MH algorithm closely match the true density of Gamma distribution.

Back to Bayesian inference, let's consider a 




## Reference
<a name="Metro">\[1\]</a> Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087–1092, 1953.

<a name="Hastings">\[2\]</a> W Keith Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97–109, 1970.